Running script with arguments: Namespace(input_channels=45, input_size=[16, 16], modality='hyper', num_classes=3, test_folder=['/mmfs1/gscratch/stf/upanpra/AK_paper_data/16crops/hardwood_vs_softwood/vis/test', '/mmfs1/gscratch/stf/upanpra/AK_paper_data/16crops/hardwood_vs_softwood/chm/test', '/mmfs1/gscratch/stf/upanpra/AK_paper_data/16crops/hardwood_vs_softwood/dtm/test'], train_folder=['/mmfs1/gscratch/stf/upanpra/AK_paper_data/16crops/hardwood_vs_softwood/vis/train', '/mmfs1/gscratch/stf/upanpra/AK_paper_data/16crops/hardwood_vs_softwood/chm/train', '/mmfs1/gscratch/stf/upanpra/AK_paper_data/16crops/hardwood_vs_softwood/dtm/train'])
tensor([ 1.2413e-03,  1.3992e+00,  1.5666e-01,  2.4665e-03,  3.7055e-03,
         2.8208e-01,  2.4620e-01,  1.1281e-02,  5.7995e+07,  3.3269e-01,
         1.0076e+00,  1.0347e+03,  1.0074e+03,  1.0008e+03,  4.0351e+00,
         2.8078e+00,  1.4801e+00,  6.4264e-01,  1.7263e+00,  1.6173e+03,
         7.6670e-01,  3.2386e-01,  2.9248e+00,  4.8894e+00,  1.6173e+03,
         7.6670e-01,  6.7660e-01, -2.4590e-02,  1.6352e-01,  6.7651e-01,
        -6.2282e-02,  1.1902e+00,  6.5222e-01,  4.6292e-02,  6.9664e+00,
         6.5256e-01,  3.7723e-01,  6.9674e+02,  8.9830e-01,  7.0955e-01,
         1.0268e+00,  6.1569e+04,  9.7964e-01,  1.4239e+00,  2.2436e+00,
         5.7266e+02])
tensor([4.2835e-04, 4.0767e-01, 6.8023e-02, 1.5014e-03, 1.7588e-03, 4.3323e-02,
        4.0901e-02, 2.5345e-03, 2.5077e+07, 2.5095e-01, 2.5432e-01, 2.0523e+02,
        1.9951e+02, 2.0219e+02, 5.4792e-01, 3.3658e-01, 1.8284e-01, 8.7965e-02,
        4.7020e-01, 3.2467e+02, 4.8012e-02, 3.8059e-02, 5.3951e-01, 9.9058e-01,
        3.2467e+02, 4.8012e-02, 4.9931e-02, 3.5875e-02, 6.8917e-02, 4.9927e-02,
        1.4947e-02, 1.0155e-01, 4.8966e-02, 1.9410e-02, 1.3976e+00, 1.7419e-01,
        3.9700e-02, 9.5890e+00, 5.9467e-02, 9.4901e-02, 3.9157e-01, 1.2540e+04,
        2.3095e-01, 7.1845e-02, 1.7827e+00, 1.8147e+00])
Saving model to: /mmfs1/gscratch/stf/upanpra/model_checkpointsAK/best-test-model-VisDroughtCNN-1700283091.616753.pt
shape of mean, torch.Size([46])
shape of std, torch.Size([46])
DroughtCNN(
  (conv1): Conv2d(45, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (conv3): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (bn3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (fc1): Linear(in_features=512, out_features=120, bias=True)
  (fc2): Linear(in_features=120, out_features=84, bias=True)
  (fc3): Linear(in_features=84, out_features=4, bias=True)
)
28338051.52741793
Traceback (most recent call last):
  File "/mmfs1/gscratch/stf/upanpra/AK_train_scripts/train_script_parameterized.py", line 151, in <module>
    main()
  File "/mmfs1/gscratch/stf/upanpra/AK_train_scripts/train_script_parameterized.py", line 123, in main
    net, train_history, test_history = trainCNN(net, train_loader, test_loader,
  File "/mmfs1/gscratch/stf/upanpra/src/trainer.py", line 40, in trainCNN
    outputs = net(images)               # Forward pass: compute the output class given a image
  File "/mmfs1/home/upanpra/miniconda3/envs/deeplearningCuda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/mmfs1/gscratch/stf/upanpra/src/model.py", line 31, in forward
    x = F.max_pool2d(F.relu(self.bn1(self.conv1(x))), kernel_size = 2, stride = 2)
  File "/mmfs1/home/upanpra/miniconda3/envs/deeplearningCuda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/mmfs1/home/upanpra/miniconda3/envs/deeplearningCuda/lib/python3.8/site-packages/torch/nn/modules/conv.py", line 447, in forward
    return self._conv_forward(input, self.weight, self.bias)
  File "/mmfs1/home/upanpra/miniconda3/envs/deeplearningCuda/lib/python3.8/site-packages/torch/nn/modules/conv.py", line 443, in _conv_forward
    return F.conv2d(input, weight, bias, self.stride,
RuntimeError: Given groups=1, weight of size [32, 45, 3, 3], expected input[16, 46, 16, 16] to have 45 channels, but got 46 channels instead
