Running script with arguments: Namespace(input_channels=44, input_size=[16, 16], modality='hyper', num_classes=3, test_folder=['/mmfs1/gscratch/stf/upanpra/AK_paper_data/16crops/hardwood_vs_softwood/vis/test'], train_folder=['/mmfs1/gscratch/stf/upanpra/AK_paper_data/16crops/hardwood_vs_softwood/vis/train'])
tensor([ 1.2413e-03,  1.3992e+00,  1.5666e-01,  2.4665e-03,  3.7055e-03,
         2.8208e-01,  2.4620e-01,  1.1281e-02,  5.7995e+07,  3.3269e-01,
         1.0076e+00,  1.0347e+03,  1.0074e+03,  1.0008e+03,  4.0351e+00,
         2.8078e+00,  1.4801e+00,  6.4264e-01,  1.7263e+00,  1.6173e+03,
         7.6670e-01,  3.2386e-01,  2.9248e+00,  4.8894e+00,  1.6173e+03,
         7.6670e-01,  6.7660e-01, -2.4590e-02,  1.6352e-01,  6.7651e-01,
        -6.2282e-02,  1.1902e+00,  6.5222e-01,  4.6292e-02,  6.9664e+00,
         6.5256e-01,  3.7723e-01,  6.9674e+02,  8.9830e-01,  7.0955e-01,
         1.0268e+00,  6.1569e+04,  9.7964e-01,  1.4239e+00])
tensor([4.2835e-04, 4.0767e-01, 6.8023e-02, 1.5014e-03, 1.7588e-03, 4.3323e-02,
        4.0901e-02, 2.5345e-03, 2.5077e+07, 2.5095e-01, 2.5432e-01, 2.0523e+02,
        1.9951e+02, 2.0219e+02, 5.4792e-01, 3.3658e-01, 1.8284e-01, 8.7965e-02,
        4.7020e-01, 3.2467e+02, 4.8012e-02, 3.8059e-02, 5.3951e-01, 9.9058e-01,
        3.2467e+02, 4.8012e-02, 4.9931e-02, 3.5875e-02, 6.8917e-02, 4.9927e-02,
        1.4947e-02, 1.0155e-01, 4.8966e-02, 1.9410e-02, 1.3976e+00, 1.7419e-01,
        3.9700e-02, 9.5890e+00, 5.9467e-02, 9.4901e-02, 3.9157e-01, 1.2540e+04,
        2.3095e-01, 7.1845e-02])
Saving model to: /mmfs1/gscratch/stf/upanpra/model_checkpointsAK/best-test-model-Visconv_next_model-1700289733.2867618.pt
ConvNeXt(
  (features): Sequential(
    (0): Conv2dNormActivation(
      (0): Conv2d(44, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (1): LayerNorm2d((128,), eps=1e-06, elementwise_affine=True)
    )
    (1): Dropout(p=0.0, inplace=False)
    (2): Sequential(
      (0): CNBlock(
        (block): Sequential(
          (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128)
          (1): Dropout(p=0.0, inplace=False)
          (2): Permute()
          (3): LayerNorm((128,), eps=1e-06, elementwise_affine=True)
          (4): Linear(in_features=128, out_features=512, bias=True)
          (5): GELU()
          (6): Linear(in_features=512, out_features=128, bias=True)
          (7): Permute()
        )
        (stochastic_depth): StochasticDepth(p=0.0, mode=row)
      )
      (1): CNBlock(
        (block): Sequential(
          (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128)
          (1): Dropout(p=0.0, inplace=False)
          (2): Permute()
          (3): LayerNorm((128,), eps=1e-06, elementwise_affine=True)
          (4): Linear(in_features=128, out_features=512, bias=True)
          (5): GELU()
          (6): Linear(in_features=512, out_features=128, bias=True)
          (7): Permute()
        )
        (stochastic_depth): StochasticDepth(p=0.0, mode=row)
      )
      (2): CNBlock(
        (block): Sequential(
          (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128)
          (1): Dropout(p=0.0, inplace=False)
          (2): Permute()
          (3): LayerNorm((128,), eps=1e-06, elementwise_affine=True)
          (4): Linear(in_features=128, out_features=512, bias=True)
          (5): GELU()
          (6): Linear(in_features=512, out_features=128, bias=True)
          (7): Permute()
        )
        (stochastic_depth): StochasticDepth(p=0.0, mode=row)
      )
    )
    (3): Sequential(
      (0): LayerNorm2d((128,), eps=1e-06, elementwise_affine=True)
      (1): Conv2d(128, 192, kernel_size=(2, 2), stride=(2, 2))
    )
    (4): Sequential(
      (0): CNBlock(
        (block): Sequential(
          (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192)
          (1): Dropout(p=0.0, inplace=False)
          (2): Permute()
          (3): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
          (4): Linear(in_features=192, out_features=768, bias=True)
          (5): GELU()
          (6): Linear(in_features=768, out_features=192, bias=True)
          (7): Permute()
        )
        (stochastic_depth): StochasticDepth(p=0.0, mode=row)
      )
    )
  )
  (avgpool): AdaptiveAvgPool2d(output_size=1)
  (final_dropout): Dropout(p=0.0, inplace=False)
  (classifier): Sequential(
    (0): LayerNorm2d((192,), eps=1e-06, elementwise_affine=True)
    (1): Flatten(start_dim=1, end_dim=-1)
    (2): Linear(in_features=192, out_features=3, bias=True)
  )
)
28338162.2220166
Epoch [1/100], Step [100/178], Loss: 0.7575
Epoch [1/100], Train Acc 62.74%, Test Acc 64.04%
Epoch [2/100], Step [100/178], Loss: 0.7334
Epoch [2/100], Train Acc 66.37%, Test Acc 63.35%
Epoch [3/100], Step [100/178], Loss: 0.6295
Epoch [3/100], Train Acc 69.41%, Test Acc 66.39%
Epoch [4/100], Step [100/178], Loss: 0.5403
Epoch [4/100], Train Acc 70.57%, Test Acc 69.71%
Epoch [5/100], Step [100/178], Loss: 0.7788
Epoch [5/100], Train Acc 69.12%, Test Acc 67.63%
Epoch [6/100], Step [100/178], Loss: 0.7682
Epoch [6/100], Train Acc 71.00%, Test Acc 67.63%
Epoch [7/100], Step [100/178], Loss: 0.9238
Epoch [7/100], Train Acc 70.75%, Test Acc 68.60%
Epoch [8/100], Step [100/178], Loss: 0.8109
Epoch [8/100], Train Acc 72.05%, Test Acc 68.60%
Epoch [9/100], Step [100/178], Loss: 1.1668
Epoch [9/100], Train Acc 72.27%, Test Acc 69.85%
Epoch [10/100], Step [100/178], Loss: 0.6519
Epoch [10/100], Train Acc 69.76%, Test Acc 65.01%
Epoch [11/100], Step [100/178], Loss: 0.5344
Epoch [11/100], Train Acc 68.07%, Test Acc 63.49%
Epoch [12/100], Step [100/178], Loss: 0.5476
Epoch [12/100], Train Acc 68.70%, Test Acc 67.91%
Epoch [13/100], Step [100/178], Loss: 0.7743
Epoch [13/100], Train Acc 72.02%, Test Acc 66.67%
Epoch [14/100], Step [100/178], Loss: 0.6154
Epoch [14/100], Train Acc 71.88%, Test Acc 68.46%
Epoch [15/100], Step [100/178], Loss: 0.6088
Epoch [15/100], Train Acc 72.34%, Test Acc 69.02%
Epoch [16/100], Step [100/178], Loss: 0.4443
Epoch [16/100], Train Acc 73.01%, Test Acc 69.71%
Epoch [17/100], Step [100/178], Loss: 0.6556
Epoch [17/100], Train Acc 70.82%, Test Acc 63.90%
Epoch [18/100], Step [100/178], Loss: 0.4076
Epoch [18/100], Train Acc 71.59%, Test Acc 69.57%
Epoch [19/100], Step [100/178], Loss: 1.0489
Epoch [19/100], Train Acc 72.94%, Test Acc 69.71%
Epoch [20/100], Step [100/178], Loss: 0.5304
Epoch [20/100], Train Acc 72.48%, Test Acc 69.71%
Epoch [21/100], Step [100/178], Loss: 0.4751
Epoch [21/100], Train Acc 73.04%, Test Acc 68.74%
Epoch [22/100], Step [100/178], Loss: 0.7691
Epoch [22/100], Train Acc 71.70%, Test Acc 67.63%
Epoch [23/100], Step [100/178], Loss: 0.7492
Epoch [23/100], Train Acc 73.54%, Test Acc 69.57%
Epoch [24/100], Step [100/178], Loss: 1.0072
Epoch [24/100], Train Acc 72.19%, Test Acc 70.26%
Epoch [25/100], Step [100/178], Loss: 0.7076
Epoch [25/100], Train Acc 71.56%, Test Acc 66.94%
Epoch [26/100], Step [100/178], Loss: 0.7194
Epoch [26/100], Train Acc 72.90%, Test Acc 68.19%
Epoch [27/100], Step [100/178], Loss: 0.8308
Epoch [27/100], Train Acc 72.34%, Test Acc 71.78%
Epoch [28/100], Step [100/178], Loss: 0.8401
Epoch [28/100], Train Acc 73.25%, Test Acc 69.02%
Epoch [29/100], Step [100/178], Loss: 0.6350
Epoch [29/100], Train Acc 73.43%, Test Acc 70.12%
Epoch [30/100], Step [100/178], Loss: 0.4937
Epoch [30/100], Train Acc 73.50%, Test Acc 69.99%
Epoch [31/100], Step [100/178], Loss: 0.4091
Epoch [31/100], Train Acc 72.94%, Test Acc 68.19%
Epoch [32/100], Step [100/178], Loss: 0.5041
Epoch [32/100], Train Acc 73.54%, Test Acc 69.43%
Epoch [33/100], Step [100/178], Loss: 0.4088
Epoch [33/100], Train Acc 72.79%, Test Acc 66.67%
Epoch [34/100], Step [100/178], Loss: 0.5544
Epoch [34/100], Train Acc 73.47%, Test Acc 69.99%
Epoch [35/100], Step [100/178], Loss: 0.5209
Epoch [35/100], Train Acc 72.90%, Test Acc 68.74%
Epoch [36/100], Step [100/178], Loss: 0.6659
Epoch [36/100], Train Acc 73.57%, Test Acc 71.65%
Epoch [37/100], Step [100/178], Loss: 0.2843
Epoch [37/100], Train Acc 73.50%, Test Acc 69.85%
Epoch [38/100], Step [100/178], Loss: 0.8142
Epoch [38/100], Train Acc 73.64%, Test Acc 69.02%
Epoch [39/100], Step [100/178], Loss: 0.5031
Epoch [39/100], Train Acc 73.61%, Test Acc 70.12%
Epoch [40/100], Step [100/178], Loss: 0.7537
Epoch [40/100], Train Acc 73.71%, Test Acc 70.40%
Epoch [41/100], Step [100/178], Loss: 0.4764
Epoch [41/100], Train Acc 74.14%, Test Acc 69.99%
Epoch [42/100], Step [100/178], Loss: 0.5369
Epoch [42/100], Train Acc 74.35%, Test Acc 69.16%
Epoch [43/100], Step [100/178], Loss: 0.5039
Epoch [43/100], Train Acc 73.92%, Test Acc 70.40%
Epoch [44/100], Step [100/178], Loss: 0.6288
Epoch [44/100], Train Acc 73.78%, Test Acc 69.29%
Epoch [45/100], Step [100/178], Loss: 0.8706
Epoch [45/100], Train Acc 73.75%, Test Acc 69.57%
Epoch [46/100], Step [100/178], Loss: 0.8529
Epoch [46/100], Train Acc 74.17%, Test Acc 70.54%
Epoch [47/100], Step [100/178], Loss: 1.0817
Epoch [47/100], Train Acc 73.71%, Test Acc 69.57%
Epoch [48/100], Step [100/178], Loss: 0.3314
Epoch [48/100], Train Acc 74.31%, Test Acc 70.26%
Epoch [49/100], Step [100/178], Loss: 0.9033
Epoch [49/100], Train Acc 74.38%, Test Acc 70.40%
Epoch [50/100], Step [100/178], Loss: 0.7572
Epoch [50/100], Train Acc 74.28%, Test Acc 69.57%
Epoch [51/100], Step [100/178], Loss: 0.5914
Epoch [51/100], Train Acc 73.01%, Test Acc 69.02%
Epoch [52/100], Step [100/178], Loss: 0.5310
Epoch [52/100], Train Acc 74.38%, Test Acc 70.40%
Epoch [53/100], Step [100/178], Loss: 0.7887
Epoch [53/100], Train Acc 74.14%, Test Acc 70.40%
Epoch [54/100], Step [100/178], Loss: 0.3682
Epoch [54/100], Train Acc 74.14%, Test Acc 69.99%
Epoch [55/100], Step [100/178], Loss: 0.8290
Epoch [55/100], Train Acc 74.45%, Test Acc 70.54%
Epoch [56/100], Step [100/178], Loss: 0.3645
Epoch [56/100], Train Acc 74.31%, Test Acc 71.37%
Epoch [57/100], Step [100/178], Loss: 0.7598
Epoch [57/100], Train Acc 73.71%, Test Acc 69.85%
Epoch [58/100], Step [100/178], Loss: 0.6496
Epoch [58/100], Train Acc 74.56%, Test Acc 70.82%
Epoch [59/100], Step [100/178], Loss: 0.3490
Epoch [59/100], Train Acc 74.42%, Test Acc 71.23%
Epoch [60/100], Step [100/178], Loss: 0.7104
Epoch [60/100], Train Acc 73.78%, Test Acc 70.54%
Epoch [61/100], Step [100/178], Loss: 0.5821
Epoch [61/100], Train Acc 74.28%, Test Acc 69.29%
Epoch [62/100], Step [100/178], Loss: 0.6559
Epoch [62/100], Train Acc 74.10%, Test Acc 69.29%
Epoch [63/100], Step [100/178], Loss: 0.4808
Epoch [63/100], Train Acc 74.81%, Test Acc 70.82%
Epoch [64/100], Step [100/178], Loss: 0.8203
Epoch [64/100], Train Acc 74.70%, Test Acc 69.57%
Epoch [65/100], Step [100/178], Loss: 0.4969
Epoch [65/100], Train Acc 74.81%, Test Acc 70.68%
Epoch [66/100], Step [100/178], Loss: 0.9597
Epoch [66/100], Train Acc 74.56%, Test Acc 70.26%
Epoch [67/100], Step [100/178], Loss: 0.5390
Epoch [67/100], Train Acc 74.74%, Test Acc 70.68%
Epoch [68/100], Step [100/178], Loss: 1.1543
Epoch [68/100], Train Acc 75.16%, Test Acc 69.71%
Epoch [69/100], Step [100/178], Loss: 0.6536
Epoch [69/100], Train Acc 74.91%, Test Acc 70.68%
Epoch [70/100], Step [100/178], Loss: 0.5864
Epoch [70/100], Train Acc 74.77%, Test Acc 70.26%
Epoch [71/100], Step [100/178], Loss: 0.5129
Epoch [71/100], Train Acc 74.95%, Test Acc 70.82%
Epoch [72/100], Step [100/178], Loss: 0.5053
Epoch [72/100], Train Acc 75.05%, Test Acc 70.95%
Epoch [73/100], Step [100/178], Loss: 0.4181
Epoch [73/100], Train Acc 75.19%, Test Acc 70.95%
Epoch [74/100], Step [100/178], Loss: 0.5848
Epoch [74/100], Train Acc 74.84%, Test Acc 69.85%
Epoch [75/100], Step [100/178], Loss: 0.2595
Epoch [75/100], Train Acc 75.23%, Test Acc 70.54%
Epoch [76/100], Step [100/178], Loss: 0.3984
Epoch [76/100], Train Acc 75.26%, Test Acc 69.85%
Epoch [77/100], Step [100/178], Loss: 0.3489
Epoch [77/100], Train Acc 75.34%, Test Acc 69.71%
Epoch [78/100], Step [100/178], Loss: 0.6229
Epoch [78/100], Train Acc 74.98%, Test Acc 70.68%
Epoch [79/100], Step [100/178], Loss: 0.3983
Epoch [79/100], Train Acc 74.91%, Test Acc 70.54%
Epoch [80/100], Step [100/178], Loss: 0.6644
Epoch [80/100], Train Acc 75.09%, Test Acc 70.54%
Epoch [81/100], Step [100/178], Loss: 0.5920
Epoch [81/100], Train Acc 74.66%, Test Acc 71.09%
Epoch [82/100], Step [100/178], Loss: 0.5243
Epoch [82/100], Train Acc 75.05%, Test Acc 70.95%
Epoch [83/100], Step [100/178], Loss: 0.5838
Epoch [83/100], Train Acc 74.98%, Test Acc 70.68%
Epoch [84/100], Step [100/178], Loss: 0.5161
Epoch [84/100], Train Acc 75.37%, Test Acc 69.85%
Epoch [85/100], Step [100/178], Loss: 0.2897
Epoch [85/100], Train Acc 75.30%, Test Acc 70.12%
Epoch [86/100], Step [100/178], Loss: 0.5552
Epoch [86/100], Train Acc 75.48%, Test Acc 69.85%
Epoch [87/100], Step [100/178], Loss: 0.5787
Epoch [87/100], Train Acc 75.09%, Test Acc 69.99%
Epoch [88/100], Step [100/178], Loss: 0.5053
Epoch [88/100], Train Acc 75.51%, Test Acc 69.85%
Epoch [89/100], Step [100/178], Loss: 0.8680
Epoch [89/100], Train Acc 75.02%, Test Acc 69.99%
Epoch [90/100], Step [100/178], Loss: 0.8455
Epoch [90/100], Train Acc 75.34%, Test Acc 70.12%
Epoch [91/100], Step [100/178], Loss: 0.3917
Epoch [91/100], Train Acc 75.19%, Test Acc 70.26%
Epoch [92/100], Step [100/178], Loss: 0.6128
Epoch [92/100], Train Acc 75.69%, Test Acc 69.99%
Epoch [93/100], Step [100/178], Loss: 0.4328
Epoch [93/100], Train Acc 75.34%, Test Acc 69.85%
Epoch [94/100], Step [100/178], Loss: 0.6395
Epoch [94/100], Train Acc 75.19%, Test Acc 69.85%
Epoch [95/100], Step [100/178], Loss: 0.6022
Epoch [95/100], Train Acc 75.37%, Test Acc 69.85%
Epoch [96/100], Step [100/178], Loss: 0.6001
Epoch [96/100], Train Acc 75.05%, Test Acc 69.99%
Epoch [97/100], Step [100/178], Loss: 0.4813
Epoch [97/100], Train Acc 75.41%, Test Acc 69.85%
Epoch [98/100], Step [100/178], Loss: 0.6271
Epoch [98/100], Train Acc 75.30%, Test Acc 69.85%
Epoch [99/100], Step [100/178], Loss: 0.7592
Epoch [99/100], Train Acc 74.95%, Test Acc 69.85%
Epoch [100/100], Step [100/178], Loss: 0.8025
Epoch [100/100], Train Acc 75.34%, Test Acc 69.85%
1700290949.8497365
 Elapsed time: 20.275479006767274 minutes
 Elapsed time: 0.3379246501127879 hours
1700290949.849745
Saved model to /mmfs1/gscratch/stf/upanpra/model_checkpointsAK/model-VIs-conv_next_model-1700289733.2867618.pt
[[ 66   5  74]
 [ 20 124  52]
 [ 23  44 315]]
[[0.45517241 0.03448276 0.51034483]
 [0.10204082 0.63265306 0.26530612]
 [0.06020942 0.11518325 0.82460733]]
[[0.60550459 0.02890173 0.16780045]
 [0.18348624 0.71676301 0.11791383]
 [0.21100917 0.25433526 0.71428571]]
